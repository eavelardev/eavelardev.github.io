# NLP history

<aside class="margin sidebar">

::::{grid}
:::{grid-item}
:::
:::{grid-item}
<div id="slide-controls" class="btn-toolbar justify-content-between">

<button id="arrow_back" class="sd-btn">{material-regular}`arrow_back_ios;1.2em`</button>

<button id="arrow_forward" class="sd-btn">{material-regular}`arrow_forward_ios;1.2em`</button>
</div>
:::
::::
</aside>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/001.jpg
:alt: "1/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">


</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/002.jpg
:alt: "2/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

The history of `NLP` can be simply divided into two eras: before and after **ML**(**machine learning**).

What divides these two eras is the innovation of using `ML` **models** to solve **NLP problems**,
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/003.jpg
:alt: "3/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

specifically **neural networks** and **deep learning** in `NLP`.
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/004.jpg
:alt: "4/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

The journey of `NLP` can be traced back to over 70 years ago in the late `1940s`.
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/005.jpg
:alt: "5/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

In `1949`, people started the idea of using machines to help translation. 

The primary methods back then were based on either **hand-coded rules** or **statistical reasoning**.
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/006.jpg
:alt: "6/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

**Natural language models** marked the innovation of applying **machine learning** in `NLP` in `2001`.

**Natural language models** use n**eural networks** to **predict** the next word, given the previous words. 

It also introduces **word embedding**, a key technique that uses **vectors** to represent given words.
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/007.jpg
:alt: "7/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

Motivated by the advances of computer hardware, **multi-task learning** appeared in `2008`. 

It allows **training models** to solve more than one learning task, such as **entity recognition** and **topic classification**, based on a set of shared parameters.
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/008.jpg
:alt: "8/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

`2013` and `2014` mark the emergence of multiple new **NLP models** or **architectures** enabled by **neural networks**. 

Some **models** produced remarkable results that are widely used today.

They are: 
* **recurrent neural networks** (`RNNs`), which were soon replaced by its variants 
* **long-short term memory** (`LSTM`) networks and 
* **Gated Recurrent Units** (`GRUs`); 
* **convolutional neural networks** (`CNNs`); and 
* **sequence-to-sequence models**.
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/009.jpg
:alt: "9/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

In `2015`, the **attention mechanism** was introduced to identify only the most relevant section instead of the entire sentence to be processed in the **neural network**.

It solves the blockage problem of the **fixed-length encoding vector** that the previous **NLP models** had. 

Additionally, it improves the **model performance** by focusing on only the most relevant information to accomplish a task.
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/010.jpg
:alt: "10/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

The **attention mechanism** generated a new **state-of-the-art architecture** called **large language models**, or **pre-trained language models**, in `2017`. 

**Large pre-trained language models** are designed to **pre-train general language models** with large amounts of **data** and **parameters**, which can be **fine-tuned** later for more specific tasks.
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/011.jpg
:alt: "11/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

This trend triggered a series of modules or libraries such as **Transformers** by **Google Brain** in `2017`,
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/012.jpg
:alt: "12/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

**Google** also created `BERT` (or **Bidirectional Encoder Representations from Transformers**) in `2018`,
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/013.jpg
:alt: "13/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

`T5` in `2019`,
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/014.jpg
:alt: "14/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

`GPT-3` (or the third-generation **Generative Pre-trained Transformer**) by **Open AI** in `2020`,
</div>
</div>
</div>
</div>
<div class="slides">
<div>

```{image} ../../../images/gcp_courses/nlp_on_gcp/nlp_on_gcp/nlp_history/015.jpg
:alt: "15/15 NLP history"
:class: slide-img
```
<div class="cell tag_remove-input tag_output_scroll docutils container">
<div class="cell_output docutils container">

and `PaLM` (or **Pathways Language Models**) by **Google**.
</div>
</div>
</div>
</div>
